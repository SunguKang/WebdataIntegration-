{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9f0f8ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "A-B; Size: 227 of which True: 28%\n",
      "A-D; Size: 209 of which True: 38%\n",
      "B-C; Size: 221 of which True: 32%\n",
      "C-D; Size: 187 of which True: 34%\n",
      "C-E; Size: 178 of which True: 37%\n",
      "Total size:  1022\n"
     ]
    }
   ],
   "source": [
    "#descriptives of gold standard\n",
    "import os\n",
    "import pandas as pd\n",
    "gold_path = \"../data/gold_standard/merged\"\n",
    "files = os.listdir(gold_path)\n",
    "counts = list()\n",
    "for i in range(len(files)):\n",
    "    print(files[i].replace(r\".csv\",\"\"), end=\"; Size: \")\n",
    "    files[i] = gold_path+\"/\"+files[i]\n",
    "    csv = pd.read_csv(files[i],header=None)\n",
    "    counts.append(csv[2].value_counts())\n",
    "    print(sum(counts[i]), end= \" of which True: \")#/(counts[1]+counts[0])\n",
    "    print(round(100*counts[i][1]/(sum(counts[i])),), end = \"%\\n\")\n",
    "print(\"Total size: \",sum([sum(c) for c in counts]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "85759809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "178\n",
      "0.3651685393258427\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "bbcf8301",
   "metadata": {},
   "outputs": [],
   "source": [
    "s1 = ['id','name','platform','publishers','publicationDate',\n",
    "      'globallySoldUnits','genres','criticScore','userScore',\n",
    "      'developers','summary','rating','series']\n",
    "list_att= [\"publishers\",\"genres\",\"developers\"]\n",
    "source_list = [\"criticScore\",\"userScore\",'summary','rating','series']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "1d5708c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PublishersEvaluationRule\n",
      "GenresEvaluationRule\n",
      "DevelopersEvaluationRule\n"
     ]
    }
   ],
   "source": [
    "load_list = \"\"\"\tList<{}> {} = getObjectListFromChildElement(node, \"{}\",\n",
    "\t\t\t\"{}\", new {}XMLReader(), provenanceInfo);\n",
    "\tgame.set{}()({});\"\"\"\n",
    "#print(load_list.format(s.capitalize(),s,s,s,s.capitalize(),s.capitalize(),s))\n",
    "\n",
    "load_nonLists = \"\"\"{} = getValueFromChildElement(node, \"{}\")\n",
    "\tif()\n",
    "\tgame.set{}();\"\"\"\n",
    "record = \"\"\"record.get{}(),\"\"\"\n",
    "append = \"\"\"\tgame.appendChild(createTextElementWithProvenance(\"{}\",\n",
    "\t\t\t\trecord.get{}(),\n",
    "\t\t\t\trecord.getMergedAttributeProvenance(Game.{}), doc));\"\"\"\n",
    "\n",
    "createElement = \"\"\"\tprotected Element create{}Element(Game record, Document doc) XXA\n",
    "\t\tElement {}Root = {}Formatter.createRootElement(doc);\n",
    "\t\t{}Root.setAttribute(\"provenance\",\n",
    "\t\t\t\trecord.getMergedAttributeProvenance(Game.{}));\n",
    "\n",
    "\t\tfor ({} {} : record.get{}()) XXA\n",
    "\t\t\t{}Root.appendChild({}Formatter\n",
    "\t\t\t\t\t.createElementFromRecord({}, doc));\n",
    "\t\tXXB\n",
    "\n",
    "\t\treturn {}Root;\n",
    "\tXXB\n",
    "\"\"\" \n",
    "for s in list_att:\n",
    "        s_up = s.replace(s[0],s[0].upper(),1)\n",
    "        print(s_up+\"EvaluationRule\")\n",
    "        #print(load_nonLists.format(s,s,s.replace(s[0],s[0].upper(),1)))\n",
    "        #print(record.format(s.replace(s[0],s[0].upper(),1)))\n",
    "        #print(append.format(s,s.replace(s[0],s[0].upper(),1),s.upper()))\n",
    "        \n",
    "#for s in list_att:\n",
    "#    s_up = s.replace(s[0],s[0].upper(),1)\n",
    "#    print(createElement.format(s_up,s,s,s,s.upper(),s_up,s[0],s_up,s,s,s[0],s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "1fb56c18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tstrategy.addAttributeFuser(Game.NAME,new NameFuserLongestString(), new NameEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.PLATFORM,new PlatformFuserLongestString(), new PlatformEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.PUBLISHERS,new PublishersFuserUnion(),new PublishersEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.PUBLICATIONDATE,new DateFuser(), new PublicationdateEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.GLOBALLYSOLDUNITS,new GloballysoldunitsFuserLongestString(), new GloballysoldunitsEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.GENRES,new GenresFuserUnion(),new GenresEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.CRITICSCORE, new CriticscoreFuserFavourSource(),new CriticscoreEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.USERSCORE, new UserscoreFuserFavourSource(),new UserscoreEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.DEVELOPERS,new DevelopersFuserUnion(),new DevelopersEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.SUMMARY, new SummaryFuserFavourSource(),new SummaryEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.RATING, new RatingFuserFavourSource(),new RatingEvaluationRule());\n",
      "\tstrategy.addAttributeFuser(Game.SERIES, new SeriesFuserFavourSource(),new SeriesEvaluationRule());\n"
     ]
    }
   ],
   "source": [
    "s1 = ['id','name','platform','publishers','publicationDate',\n",
    "      'globallySoldUnits','genres','criticScore','userScore',\n",
    "      'developers','summary','rating','series']\n",
    "list_att= [\"publishers\",\"genres\",\"developers\"]\n",
    "source_list = [\"criticScore\",\"userScore\",'summary','rating','series']\n",
    "#s1 = s1.split(\"\\n\")\n",
    "sc=\"dataset.addAttribute(Movie.{});\"\n",
    "sc2 = 'public static final Attribute {} = new Attribute(\"{}\");'\t\n",
    "sc3 = '''else if(attribute=={})\n",
    "\treturn get{}() != null && !get{}().isEmpty();'''\n",
    "sc4='''else if(attribute=={})\n",
    "\treturn get{}() != null && get{}().size() > 0;'''\n",
    "fusion_logest='\tstrategy.addAttributeFuser(Game.{},new {}FuserLongestString(), new {}EvaluationRule());'\n",
    "fusion_newest='\tstrategy.addAttributeFuser(Game.{},new DateFuser(), new {}EvaluationRule());'\n",
    "fusion_source='\tstrategy.addAttributeFuser(Game.{}, new {}FuserFavourSource(),new {}EvaluationRule());'\n",
    "fusion_union ='\tstrategy.addAttributeFuser(Game.{},new {}FuserUnion(),new {}EvaluationRule());'\n",
    "for s in s1[1:]:\n",
    "    #print(sc.format(s.upper()))\n",
    "    #print(sc2.format(s.upper(),s))\n",
    "    if \"publicationDate\" == s:\n",
    "        print(fusion_newest.format(s.upper(),s.capitalize()))\n",
    "        continue\n",
    "    if s in list_att:\n",
    "        print(fusion_union.format(s.upper(),s.capitalize(),s.capitalize()))\n",
    "        continue\n",
    "    if s in source_list:\n",
    "        print(fusion_source.format(s.upper(),s.capitalize(),s.capitalize()))\n",
    "        continue\n",
    "    print(fusion_logest.format(s.upper(),s.capitalize(),s.capitalize()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ff16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#path to folder with \n",
    "path_xml = r\"..\\data\\schema_mapping\\integrated_target_schema_xml\".replace(\"\\\\\",\"/\")\n",
    "path_mapping = r\"../data/schema_mapping\"\n",
    "path_scema_csv = r\"../data/schema_mapping/integrated_target_schema_csv\"\n",
    "gold_path = r\"../data/gold_standard/gold_standard_leon\"\n",
    "#the paths to the original .csv datasets  \n",
    "paths = [r\"integrated_target_schema_Windows.csv\"\n",
    ",r\"integrated_target_schemaPS4.csv\"\n",
    ",r\"target_schema_metacritic.csv\"\n",
    ",r\"target_schema_Video_Games_Sales.csv\"\n",
    ",r\"wikidata_integrated_target_schema.csv\"]\n",
    "\n",
    "#path to godl folder\n",
    "prePro_path = r\"../data/preprocessing\".replace(\"\\\\\",\"/\")\n",
    "#the paths to the matches\n",
    "c_paths= [r\"A-B.csv\",\n",
    "r\"A-D.csv\",\n",
    "r\"B-C.csv\",\n",
    "r\"C-D.csv\",\n",
    "r\"C-E.csv\"]\n",
    "\n",
    "#preprocessing the pathnames for gold_stadard\n",
    "for i in range(0,len(paths)):\n",
    "    paths[i] = path_scema_csv+\"/\"+paths[i]\n",
    "\n",
    "for i in range(0,len(c_paths)):\n",
    "    c_paths[i] = gold_path+\"/\"+c_paths[i]\n",
    "\n",
    "#Naming the paths\n",
    "names = [\"B\",\"D\",\"A\",\"C\",\"E\"]\n",
    "paths = dict(zip(names,paths))\n",
    "compare = [[\"A\",\"B\"],[\"A\",\"D\"],[\"B\",\"C\"],[\"C\",\"D\"],[\"C\",\"E\"]]\n",
    "\n",
    "#function that creates .csv files from .xml files\n",
    "\n",
    "def xml_to_csv(path_source_folder, path_target_folder = \"\", attributes = ['id','name','platform','publishers','publicationDate',\n",
    "      'globallySoldUnits','genres','criticScore','userScore',\n",
    "      'developers','summary','rating','series'], list_att= [\"publishers\",\"genres\",\"developers\"] ):\n",
    "        '''\n",
    "        path_source_folder : is the path to the folder with the .xml files\n",
    "        attributes : list of names of the attributes in the .xml files\n",
    "        list_att : list of names of the attributes in attributes which are list attributes \n",
    "        '''\n",
    "        path_source_folder = path_source_folder.replace(\"\\\\\",\"/\")\n",
    "        names = [name for name in os.listdir(path_source_folder) if \".xml\" in name]\n",
    "        paths = [path_source_folder+\"/\"+name for name in names]\n",
    "\n",
    "        if (\"\"==path_target_folder):\n",
    "            path_target_folder = path_source_folder\n",
    "\n",
    "        for i,f in enumerate(paths):\n",
    "            # PARSE XML\n",
    "            #print(f)\n",
    "            xml = ElementTree.parse(f)\n",
    "\n",
    "            # CREATE CSV FILE\n",
    "            with open(path_target_folder + \"/\" + names[i].replace(\".xml\",\".csv\"), 'w', newline='',encoding=\"utf-8\") as outfile:\n",
    "                writer = csv.writer(outfile)\n",
    "                #csvfile = open(f.replace(\".xml\",\".csv\"),'w',encoding='utf-8')\n",
    "                #csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "                # ADD THE HEADER TO CSV FILE\n",
    "                #csvfile_writer.writerow([\"id\",\"name\"])\n",
    "\n",
    "                writer.writerow(attributes)\n",
    "\n",
    "                # FOR EACH EMPLOYEE\n",
    "                for i, videogame in enumerate(xml.findall(\"videogame\")):\n",
    "                    if(videogame):\n",
    "                        csv_line = list()\n",
    "                        for a in attributes:\n",
    "                                locals()[a] = videogame.find(a)\n",
    "                                #print(\"bool\",a,\":\", None != locals()[a])\n",
    "                                if (None != locals()[a]):\n",
    "                                    if (a in list_att):\n",
    "                                        sub_list = list()\n",
    "                                        for c1 in locals()[a].findall(\"*\"):\n",
    "                                            for c2 in c1.findall(\"*\"):\n",
    "                                                sub_list.append(c2.text)\n",
    "                                        csv_line.append(\",\".join(sub_list))\n",
    "                                    else: \n",
    "                                        csv_line.append(locals()[a].text)\n",
    "                                else:\n",
    "                                    csv_line.append(\"\")\n",
    "                                    #print(\"line after append:\",csv_line)\n",
    "                        # ADD A NEW ROW TO CSV FILE\n",
    "                        #csvfile_writer.writerow(csv_line)\n",
    "                        writer.writerow(csv_line)\n",
    "                        #print(\"final line:\",csv_line)\n",
    "                #csvfile.close()\n",
    "                \n",
    "#create .csv Files from the .xml files using the function\n",
    "xml_to_csv(path_source_folder=path_xml, path_target_folder=path_scema_csv)\n",
    "#save integrated scemas also as Dataset_X.csv (make shure the subfolder \"integratet_target_schema_csv\" exists)\n",
    "for n in names:\n",
    "    csv = pd.read_csv(paths[n])\n",
    "    csv.to_csv(path_scema_csv+\"/Dataset_\"+n+\".csv\", sep=\";\", index=False)\n",
    "    \n",
    "\n",
    "#find all platform names in dataset\n",
    "\n",
    "#all_platforms = [set(pd.read_csv(p)[\"platform\"]) for p in paths.values()]\n",
    "#export them\n",
    "#pd.DataFrame(all_platforms).to_csv(path_to_folder+ \"/\" + \"platforms\"+\".csv\",index=False, sep=\";\")  \n",
    "#reimport them (after they were edited and matched by hand)\n",
    "platforms = pd.read_csv(prePro_path+\"/platforms.csv\", sep =\";\", header=None)\n",
    "#transpose\n",
    "platforms = platforms.transpose()\n",
    "#select ony the ones in c (imputed for the ones in a)\n",
    "platforms = platforms.loc[~platforms.isnull()[3],range(0,5)]\n",
    "#drop the first row\n",
    "platforms = platforms.drop(0)\n",
    "#replace strings in with string lists\n",
    "for j in range (0,len(platforms.columns)):\n",
    "    for i in range(0,len(platforms)):\n",
    "        try:\n",
    "            platforms.iloc[i,j] = platforms.iloc[i,j].split(\",\")\n",
    "        except AttributeError:\n",
    "            if not math.isnan(platforms.iloc[i,j]):\n",
    "                print(\"col:\",j,\"row:\",i, platforms.iloc[i,j])    \n",
    "    \n",
    "\n",
    "#change platform names, only keep the ones present in A and C (platforms[3])\n",
    "#for all datesets\n",
    "for i in range(0,5):\n",
    "    csv = pd.read_csv(paths[names[i]])\n",
    "#for all platform names\n",
    "    for j in range(0,len(platforms)):\n",
    "    #if the platform is in this dataframe\n",
    "        if type(platforms.iloc[j,i]) == list:\n",
    "            #find matching platform name and replace with entrie from dataset C\n",
    "            csv[\"platform\"] = np.where(csv[\"platform\"].isin(platforms.iloc[j,i]),platforms.iloc[j,3],csv[\"platform\"])\n",
    "    #delete from dataset5\n",
    "    if(4==i):\n",
    "        p = [item for sublist in platforms[3] for item in sublist]\n",
    "        csv = csv[csv[\"platform\"].isin(p)]\n",
    "    #save as _uni_plat.csv\n",
    "    csv.to_csv(prePro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".csv\", index=False, sep=\";\")\n",
    "\n",
    "\n",
    "##creation of convinience files\n",
    "\n",
    "\n",
    "#combine a sample of a fith with another sample (the j+1 to fith), this can be done to create nonmatches for the gold standard\n",
    "#make shure, prePro_path has a subfolder \"random_draws\"\n",
    "for j in range(5):\n",
    "    for i in range(0,len(compare)):\n",
    "            num = 12\n",
    "            csv_1 = pd.read_csv(paths[compare[i][0]])\n",
    "            csv_2 = pd.read_csv(paths[compare[i][1]])\n",
    "            sample1 = csv_1.iloc[random.sample(range(j*csv_1.shape[0]//5,(((j+1)*csv_1.shape[0]//5)-1)),num)]\n",
    "            sample2 = csv_2.iloc[random.sample(range(0,csv_2.shape[0]),num)]\n",
    "            pd.concat([sample1.reset_index(drop=True),\n",
    "                       sample2.reset_index(drop=True)],axis=1).to_csv(gold_path+\n",
    "                                                                      \"/../random_combinations/\" +\n",
    "                                                                      \"-\".join(compare[i]) +\n",
    "                                                                      \"_\"+str(num)+\"_random_combinations\"+\n",
    "                                                                      str(j+1)+\"_fith.csv\",\n",
    "                                                                      index=False, sep =\";\")  \n",
    "#look up found matches from goldstandard files and put their rows in a table\n",
    "for i in range(0,len(c_paths)):               \n",
    "    csv_1 = pd.read_csv(paths[compare[i][0]])\n",
    "    csv_2 = pd.read_csv(paths[compare[i][1]])\n",
    "    compare_csv =  pd.read_csv(c_paths[i],sep=\";\")\n",
    "    merged_csv = pd.DataFrame(np.zeros((0,0)))\n",
    "    for j in range(0,len(compare_csv)):\n",
    "        temprow1 = csv_1[compare_csv.iloc[j][0] == csv_1[\"id\"]]\n",
    "        temprow2 = csv_2[compare_csv.iloc[j][1] == csv_2[\"id\"]]\n",
    "        merged =pd.concat([temprow1.reset_index(drop=True),temprow2.reset_index(drop=True)], axis=1)\n",
    "        merged_csv = pd.concat([merged_csv,merged], axis=0)\n",
    "    #save as dataset1-dataset2_new.csv\n",
    "    merged_csv.to_csv(c_paths[i].replace(r\".csv\", \"_full_rows.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "da063fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new version\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "#path to folder with \n",
    "path_xml = r\"..\\data\\schema_mapping\\integrated_target_schema_xml\".replace(\"\\\\\",\"/\")\n",
    "path_mapping = r\"../data/schema_mapping\"\n",
    "path_schema_csv = r\"../data/schema_mapping/integrated_target_schema_csv\"\n",
    "gold_path = r\"../data/gold_standard/gold_standard_leon\"\n",
    "#the paths to the original .csv datasets  \n",
    "paths = [r\"integrated_target_schema_Windows.csv\"\n",
    ",r\"integrated_target_schemaPS4.csv\"\n",
    ",r\"target_schema_metacritic.csv\"\n",
    ",r\"target_schema_Video_Games_Sales.csv\"\n",
    ",r\"wikidata_integrated_target_schema.csv\"]\n",
    "\n",
    "#path to godl folder\n",
    "pre_pro_path = r\"../data/preprocessing\".replace(\"\\\\\",\"/\")\n",
    "\n",
    "#the paths to the matches\n",
    "c_paths= [r\"A-B.csv\",\n",
    "r\"A-D.csv\",\n",
    "r\"B-C.csv\",\n",
    "r\"C-D.csv\",\n",
    "r\"C-E.csv\"]\n",
    "\n",
    "#preprocessing the pathnames for gold_stadard\n",
    "for i in range(0,len(paths)):\n",
    "    paths[i] = path_schema_csv+\"/\"+paths[i]\n",
    "\n",
    "for i in range(0,len(c_paths)):\n",
    "    c_paths[i] = gold_path+\"/\"+c_paths[i]\n",
    "\n",
    "#Naming the paths\n",
    "names = [\"B\",\"D\",\"A\",\"C\",\"E\"]\n",
    "paths = dict(zip(names,paths))\n",
    "compare = [[\"A\",\"B\"],[\"A\",\"D\"],[\"B\",\"C\"],[\"C\",\"D\"],[\"C\",\"E\"]]\n",
    "\n",
    "#function that creates .csv files from .xml files\n",
    "\n",
    "def xml_to_csv(path_source_folder, path_target_folder = \"\", attributes = ['id','name','platform','publishers','publicationDate',\n",
    "      'globallySoldUnits','genres','criticScore','userScore',\n",
    "      'developers','summary','rating','series'], list_att= [\"publishers\",\"genres\",\"developers\"] ):\n",
    "        '''\n",
    "        path_source_folder : is the path to the folder with the .xml files\n",
    "        attributes : list of names of the attributes in the .xml files\n",
    "        list_att : list of names of the attributes in attributes which are list attributes \n",
    "        '''\n",
    "        path_source_folder = path_source_folder.replace(\"\\\\\",\"/\")\n",
    "        names = [name for name in os.listdir(path_source_folder) if \".xml\" in name]\n",
    "        paths = [path_source_folder+\"/\"+name for name in names]\n",
    "\n",
    "        if (\"\"==path_target_folder):\n",
    "            path_target_folder = path_source_folder\n",
    "\n",
    "        for i,f in enumerate(paths):\n",
    "            # PARSE XML\n",
    "            #print(f)\n",
    "            xml = ElementTree.parse(f)\n",
    "\n",
    "            # CREATE CSV FILE\n",
    "            with open(path_target_folder + \"/\" + names[i].replace(\".xml\",\".csv\"), 'w', newline='',encoding=\"utf-8\") as outfile:\n",
    "                writer = csv.writer(outfile)\n",
    "                #csvfile = open(f.replace(\".xml\",\".csv\"),'w',encoding='utf-8')\n",
    "                #csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "                # ADD THE HEADER TO CSV FILE\n",
    "                #csvfile_writer.writerow([\"id\",\"name\"])\n",
    "\n",
    "                writer.writerow(attributes)\n",
    "\n",
    "                # FOR EACH EMPLOYEE\n",
    "                for i, videogame in enumerate(xml.findall(\"videogame\")):\n",
    "                    if(videogame):\n",
    "                        csv_line = list()\n",
    "                        for a in attributes:\n",
    "                                locals()[a] = videogame.find(a)\n",
    "                                #print(\"bool\",a,\":\", None != locals()[a])\n",
    "                                if (None != locals()[a]):\n",
    "                                    if (a in list_att):\n",
    "                                        sub_list = list()\n",
    "                                        for c1 in locals()[a].findall(\"*\"):\n",
    "                                            for c2 in c1.findall(\"*\"):\n",
    "                                                sub_list.append(c2.text)\n",
    "                                        csv_line.append(\",\".join(sub_list))\n",
    "                                    else: \n",
    "                                        csv_line.append(locals()[a].text)\n",
    "                                else:\n",
    "                                    csv_line.append(\"\")\n",
    "                                    #print(\"line after append:\",csv_line)\n",
    "                        # ADD A NEW ROW TO CSV FILE\n",
    "                        #csvfile_writer.writerow(csv_line)\n",
    "                        writer.writerow(csv_line)\n",
    "                        #print(\"final line:\",csv_line)\n",
    "                #csvfile.close()\n",
    "                \n",
    "#create .csv Files from the .xml files using the function\n",
    "xml_to_csv(path_source_folder=path_xml, path_target_folder=path_schema_csv)\n",
    "#save integrated scemas also as Dataset_X.csv (make shure the subfolder \"integratet_target_schema_csv\" exists)\n",
    "for n in names:\n",
    "    csv = pd.read_csv(paths[n])\n",
    "    csv.to_csv(path_schema_csv+\"/Dataset_\"+n+\".csv\", sep=\";\", index=False)\n",
    "    \n",
    "\n",
    "#find all platform names in dataset\n",
    "\n",
    "#all_platforms = [set(pd.read_csv(p)[\"platform\"]) for p in paths.values()]\n",
    "#export them\n",
    "#pd.DataFrame(all_platforms).to_csv(path_to_folder+ \"/\" + \"platforms\"+\".csv\",index=False, sep=\";\")  \n",
    "#reimport them (after they were edited and matched by hand)\n",
    "platforms = pd.read_csv(pre_pro_path+\"/platforms.csv\", sep =\";\", header=None)\n",
    "\n",
    "#transpose\n",
    "platforms = platforms.transpose()\n",
    "#select ony the ones in c (imputed for the ones in a)\n",
    "platforms = platforms.loc[~platforms.isnull()[3],range(0,5)]\n",
    "#drop the first row\n",
    "platforms = platforms.drop(0)\n",
    "#replace strings in with string lists\n",
    "for j in range (0,len(platforms.columns)):\n",
    "    for i in range(0,len(platforms)):\n",
    "        try:\n",
    "            platforms.iloc[i,j] = platforms.iloc[i,j].split(\",\")\n",
    "        except AttributeError:\n",
    "            if not math.isnan(platforms.iloc[i,j]):\n",
    "                print(\"ERROR was not detected as string, col:\",j,\"row:\",i, platforms.iloc[i,j])    \n",
    "    \n",
    "\n",
    "#change platform names, only keep the ones present in A and C (platforms[3])\n",
    "#for all datesets\n",
    "for i in range(len(names)):\n",
    "    data_csv = pd.read_csv(paths[names[i]])\n",
    "    #for all platform names\n",
    "    for j in range(0,len(platforms)):\n",
    "    #if the platform is in this dataframe\n",
    "        if type(platforms.iloc[j,i]) == list:\n",
    "            #find matching platform name and replace with entry from dataset C\n",
    "            data_csv[\"platform\"] = np.where(data_csv[\"platform\"].isin(platforms.iloc[j,i]),platforms.iloc[j,3],data_csv[\"platform\"])\n",
    "    #delete from dataset5\n",
    "    if(\"E\"== names[i]):\n",
    "        p = [item for sublist in platforms[3] for item in sublist]\n",
    "        cdata_csvsv = data_csv[data_csv[\"platform\"].isin(p)]\n",
    "    #############################################################\n",
    "    # TODO insert other preprocessing here \n",
    "    \n",
    "    \n",
    "    ##############################################################\n",
    "    #TODO save in folder preprocessed_csv\n",
    "    #data_csv.to_csv(pre_pro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".csv\", index=False, sep=\";\")\n",
    "    data_csv.to_xml(pre_pro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".xml\", index=False, elem_cols=[\"publishers\",\"genres\",\"developers\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "16c623b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#change platform names, only keep the ones present in A and C (platforms[3])\n",
    "#for all datesets\n",
    "for i in range(len(names)):\n",
    "    data_csv = pd.read_csv(paths[names[i]])\n",
    "    #for all platform names\n",
    "    for j in range(0,len(platforms)):\n",
    "    #if the platform is in this dataframe\n",
    "        if type(platforms.iloc[j,i]) == list:\n",
    "            #find matching platform name and replace with entry from dataset C\n",
    "            data_csv[\"platform\"] = np.where(data_csv[\"platform\"].isin(platforms.iloc[j,i]),platforms.iloc[j,3],data_csv[\"platform\"])\n",
    "    #delete from dataset5\n",
    "    if(\"E\"== names[i]):\n",
    "        p = [item for sublist in platforms[3] for item in sublist]\n",
    "        cdata_csvsv = data_csv[data_csv[\"platform\"].isin(p)]\n",
    "    #############################################################\n",
    "    # TODO insert other preprocessing here \n",
    "    \n",
    "    \n",
    "    ##############################################################\n",
    "    #TODO save in folder preprocessed_csv\n",
    "    #data_csv.to_csv(pre_pro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".csv\", index=False, sep=\";\")\n",
    "    data_csv.to_xml(pre_pro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".xml\",\n",
    "                    index=False, elem_cols= ['id',\n",
    "                    'name','platform','publishers','publicationDate','globallySoldUnits','genres',\n",
    "                                             'criticScore','userScore',\n",
    "                    'developers','summary','rating','series',\"publishers\",\"genres\",\"developers\"],\n",
    "                    root_name=\"videogames\", row_name = \"videogame\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a36f242a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C Leon [7774, 16810]\n",
      "C Sunggu [5872, 151]\n",
      "C FLo  [11657, 9124, 16644]\n",
      "C Lara [14427, 9247, 6532]\n"
     ]
    }
   ],
   "source": [
    "import random \n",
    "#print(\"B\",random.sample(range(3817),3))\n",
    "print(\"C Leon\",random.sample(range(17000),2))\n",
    "print(\"C Sunggu\",random.sample(range(17000),2))\n",
    "print(\"C FLo \",random.sample(range(17000),3))\n",
    "print(\"C Lara\",random.sample(range(17000),3))\n",
    "#print(\"D\",random.sample(range(9686),3))\n",
    "#print(\"E\",random.sample(range(90151),3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "106e9158",
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def build_item_xml(row):\n",
    "    item1 = ET.SubElement(items, 'Item')\n",
    "    descriptors = ET.SubElement(item1, 'Descriptors')\n",
    "    barcode= ET.SubElement(descriptors, 'Barcode')\n",
    "    barcode.text=row[\"Descriptors.Barcode\"]\n",
    "\n",
    "    pricing = ET.SubElement(item1, 'Pricing')\n",
    "    packetcost= ET.SubElement(pricing, 'PackCost')\n",
    "    packetcost.text=str(row[\"Pricing.PackCost\"])  # cast as without error cannot serialize 0.5625 (type float)\n",
    "    # etc\n",
    "\n",
    "    # add other attributes here\n",
    "\n",
    "    # always return a result\n",
    "    return row\n",
    "\n",
    "# mock dataframe with 2 rows based on columns supplied\n",
    "df = pd.DataFrame({\n",
    "    \"Descriptors.Barcode\": [\"9770307017919\", \"9770307017920\"],\n",
    "    \"Descriptors.SupplierCode\": [\"030701791\", \"030701792\"],\n",
    "    \"Descriptors.Description\": [\"Daily Express (Mon)\", \"Daily Express (Tues)\"],\n",
    "    \"Descriptors.CommodityGroup\": [1,2],\n",
    "    \"Pricing.PackCost\": [0.5625, 0.5626],\n",
    "    \"Pricing.CostPricePerUnit\": [0.5625, 0.5626],\n",
    "    \"Pricing.RetailPrice\": [0.75, 0.75],\n",
    "    \"Pricing.ValidFrom\": [44193, 44194],\n",
    "    \"Sizing.Packsize\": [1, 2],\n",
    "})\n",
    "\n",
    "# https://docs.python.org/3/library/xml.etree.elementtree.html#building-xml-documents\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "items = ET.Element('Items')\n",
    "\n",
    "df = df.apply(build_item_xml, axis=1). # this calls build_item_xml per row\n",
    "\n",
    "ET.dump(items)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
