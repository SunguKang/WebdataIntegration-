{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e8ff16e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "import random\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#path to folder with \n",
    "path_xml = r\"..\\data\\schema_mapping\\integrated_target_schema_xml\".replace(\"\\\\\",\"/\")\n",
    "path_mapping = r\"../data/schema_mapping\"\n",
    "path_scema_csv = r\"../data/schema_mapping/integrated_target_schema_csv\"\n",
    "gold_path = r\"../data/gold_standard/gold_standard_leon\"\n",
    "#the paths to the original .csv datasets  \n",
    "paths = [r\"integrated_target_schema_Windows.csv\"\n",
    ",r\"integrated_target_schemaPS4.csv\"\n",
    ",r\"target_schema_metacritic.csv\"\n",
    ",r\"target_schema_Video_Games_Sales.csv\"\n",
    ",r\"wikidata_integrated_target_schema.csv\"]\n",
    "\n",
    "#path to godl folder\n",
    "prePro_path = r\"../data/preprocessing\".replace(\"\\\\\",\"/\")\n",
    "#the paths to the matches\n",
    "c_paths= [r\"A-B.csv\",\n",
    "r\"A-D.csv\",\n",
    "r\"B-C.csv\",\n",
    "r\"C-D.csv\",\n",
    "r\"C-E.csv\"]\n",
    "\n",
    "#preprocessing the pathnames for gold_stadard\n",
    "for i in range(0,len(paths)):\n",
    "    paths[i] = path_scema_csv+\"/\"+paths[i]\n",
    "\n",
    "for i in range(0,len(c_paths)):\n",
    "    c_paths[i] = gold_path+\"/\"+c_paths[i]\n",
    "\n",
    "#Naming the paths\n",
    "names = [\"B\",\"D\",\"A\",\"C\",\"E\"]\n",
    "paths = dict(zip(names,paths))\n",
    "compare = [[\"A\",\"B\"],[\"A\",\"D\"],[\"B\",\"C\"],[\"C\",\"D\"],[\"C\",\"E\"]]\n",
    "\n",
    "#function that creates .csv files from .xml files\n",
    "\n",
    "def xml_to_csv(path_source_folder, path_target_folder = \"\", attributes = ['id','name','platform','publishers','publicationDate',\n",
    "      'globallySoldUnits','genres','criticScore','userScore',\n",
    "      'developers','summary','rating','series'], list_att= [\"publishers\",\"genres\",\"developers\"] ):\n",
    "        '''\n",
    "        path_source_folder : is the path to the folder with the .xml files\n",
    "        attributes : list of names of the attributes in the .xml files\n",
    "        list_att : list of names of the attributes in attributes which are list attributes \n",
    "        '''\n",
    "        path_source_folder = path_source_folder.replace(\"\\\\\",\"/\")\n",
    "        names = [name for name in os.listdir(path_source_folder) if \".xml\" in name]\n",
    "        paths = [path_source_folder+\"/\"+name for name in names]\n",
    "\n",
    "        if (\"\"==path_target_folder):\n",
    "            path_target_folder = path_source_folder\n",
    "\n",
    "        for i,f in enumerate(paths):\n",
    "            # PARSE XML\n",
    "            #print(f)\n",
    "            xml = ElementTree.parse(f)\n",
    "\n",
    "            # CREATE CSV FILE\n",
    "            with open(path_target_folder + \"/\" + names[i].replace(\".xml\",\".csv\"), 'w', newline='',encoding=\"utf-8\") as outfile:\n",
    "                writer = csv.writer(outfile)\n",
    "                #csvfile = open(f.replace(\".xml\",\".csv\"),'w',encoding='utf-8')\n",
    "                #csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "                # ADD THE HEADER TO CSV FILE\n",
    "                #csvfile_writer.writerow([\"id\",\"name\"])\n",
    "\n",
    "                writer.writerow(attributes)\n",
    "\n",
    "                # FOR EACH EMPLOYEE\n",
    "                for i, videogame in enumerate(xml.findall(\"videogame\")):\n",
    "                    if(videogame):\n",
    "                        csv_line = list()\n",
    "                        for a in attributes:\n",
    "                                locals()[a] = videogame.find(a)\n",
    "                                #print(\"bool\",a,\":\", None != locals()[a])\n",
    "                                if (None != locals()[a]):\n",
    "                                    if (a in list_att):\n",
    "                                        sub_list = list()\n",
    "                                        for c1 in locals()[a].findall(\"*\"):\n",
    "                                            for c2 in c1.findall(\"*\"):\n",
    "                                                sub_list.append(c2.text)\n",
    "                                        csv_line.append(\",\".join(sub_list))\n",
    "                                    else: \n",
    "                                        csv_line.append(locals()[a].text)\n",
    "                                else:\n",
    "                                    csv_line.append(\"\")\n",
    "                                    #print(\"line after append:\",csv_line)\n",
    "                        # ADD A NEW ROW TO CSV FILE\n",
    "                        #csvfile_writer.writerow(csv_line)\n",
    "                        writer.writerow(csv_line)\n",
    "                        #print(\"final line:\",csv_line)\n",
    "                #csvfile.close()\n",
    "                \n",
    "#create .csv Files from the .xml files using the function\n",
    "xml_to_csv(path_source_folder=path_xml, path_target_folder=path_scema_csv)\n",
    "#save integrated scemas also as Dataset_X.csv (make shure the subfolder \"integratet_target_schema_csv\" exists)\n",
    "for n in names:\n",
    "    csv = pd.read_csv(paths[n])\n",
    "    csv.to_csv(path_scema_csv+\"/Dataset_\"+n+\".csv\", sep=\";\", index=False)\n",
    "    \n",
    "\n",
    "#find all platform names in dataset\n",
    "\n",
    "#all_platforms = [set(pd.read_csv(p)[\"platform\"]) for p in paths.values()]\n",
    "#export them\n",
    "#pd.DataFrame(all_platforms).to_csv(path_to_folder+ \"/\" + \"platforms\"+\".csv\",index=False, sep=\";\")  \n",
    "#reimport them (after they were edited and matched by hand)\n",
    "platforms = pd.read_csv(prePro_path+\"/platforms.csv\", sep =\";\", header=None)\n",
    "#transpose\n",
    "platforms = platforms.transpose()\n",
    "#select ony the ones in c (imputed for the ones in a)\n",
    "platforms = platforms.loc[~platforms.isnull()[3],range(0,5)]\n",
    "#drop the first row\n",
    "platforms = platforms.drop(0)\n",
    "#replace strings in with string lists\n",
    "for j in range (0,len(platforms.columns)):\n",
    "    for i in range(0,len(platforms)):\n",
    "        try:\n",
    "            platforms.iloc[i,j] = platforms.iloc[i,j].split(\",\")\n",
    "        except AttributeError:\n",
    "            if not math.isnan(platforms.iloc[i,j]):\n",
    "                print(\"col:\",j,\"row:\",i, platforms.iloc[i,j])    \n",
    "    \n",
    "\n",
    "#change platform names, only keep the ones present in A and C (platforms[3])\n",
    "#for all datesets\n",
    "for i in range(0,5):\n",
    "    csv = pd.read_csv(paths[names[i]])\n",
    "#for all platform names\n",
    "    for j in range(0,len(platforms)):\n",
    "    #if the platform is in this dataframe\n",
    "        if type(platforms.iloc[j,i]) == list:\n",
    "            #find matching platform name and replace with entrie from dataset C\n",
    "            csv[\"platform\"] = np.where(csv[\"platform\"].isin(platforms.iloc[j,i]),platforms.iloc[j,3],csv[\"platform\"])\n",
    "    #delete from dataset5\n",
    "    if(4==i):\n",
    "        p = [item for sublist in platforms[3] for item in sublist]\n",
    "        csv = csv[csv[\"platform\"].isin(p)]\n",
    "    #save as _uni_plat.csv\n",
    "    csv.to_csv(prePro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".csv\", index=False, sep=\";\")\n",
    "\n",
    "\n",
    "##creation of convinience files\n",
    "\n",
    "\n",
    "#combine a sample of a fith with another sample (the j+1 to fith), this can be done to create nonmatches for the gold standard\n",
    "#make shure, prePro_path has a subfolder \"random_draws\"\n",
    "for j in range(5):\n",
    "    for i in range(0,len(compare)):\n",
    "            num = 12\n",
    "            csv_1 = pd.read_csv(paths[compare[i][0]])\n",
    "            csv_2 = pd.read_csv(paths[compare[i][1]])\n",
    "            sample1 = csv_1.iloc[random.sample(range(j*csv_1.shape[0]//5,(((j+1)*csv_1.shape[0]//5)-1)),num)]\n",
    "            sample2 = csv_2.iloc[random.sample(range(0,csv_2.shape[0]),num)]\n",
    "            pd.concat([sample1.reset_index(drop=True),\n",
    "                       sample2.reset_index(drop=True)],axis=1).to_csv(gold_path+\n",
    "                                                                      \"/../random_combinations/\" +\n",
    "                                                                      \"-\".join(compare[i]) +\n",
    "                                                                      \"_\"+str(num)+\"_random_combinations\"+\n",
    "                                                                      str(j+1)+\"_fith.csv\",\n",
    "                                                                      index=False, sep =\";\")  \n",
    "#look up found matches from goldstandard files and put their rows in a table\n",
    "for i in range(0,len(c_paths)):               \n",
    "    csv_1 = pd.read_csv(paths[compare[i][0]])\n",
    "    csv_2 = pd.read_csv(paths[compare[i][1]])\n",
    "    compare_csv =  pd.read_csv(c_paths[i],sep=\";\")\n",
    "    merged_csv = pd.DataFrame(np.zeros((0,0)))\n",
    "    for j in range(0,len(compare_csv)):\n",
    "        temprow1 = csv_1[compare_csv.iloc[j][0] == csv_1[\"id\"]]\n",
    "        temprow2 = csv_2[compare_csv.iloc[j][1] == csv_2[\"id\"]]\n",
    "        merged =pd.concat([temprow1.reset_index(drop=True),temprow2.reset_index(drop=True)], axis=1)\n",
    "        merged_csv = pd.concat([merged_csv,merged], axis=0)\n",
    "    #save as dataset1-dataset2_new.csv\n",
    "    merged_csv.to_csv(c_paths[i].replace(r\".csv\", \"_full_rows.csv\"), index=False, sep=\";\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da063fa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#new version\n",
    "import pandas as pd\n",
    "import numpy as np \n",
    "\n",
    "import csv\n",
    "import math\n",
    "import os\n",
    "from xml.etree import ElementTree\n",
    "\n",
    "#path to folder with \n",
    "path_xml = r\"..\\data\\schema_mapping\\integrated_target_schema_xml\".replace(\"\\\\\",\"/\")\n",
    "path_mapping = r\"../data/schema_mapping\"\n",
    "path_schema_csv = r\"../data/schema_mapping/integrated_target_schema_csv\"\n",
    "gold_path = r\"../data/gold_standard/gold_standard_leon\"\n",
    "#the paths to the original .csv datasets  \n",
    "paths = [r\"integrated_target_schema_Windows.csv\"\n",
    ",r\"integrated_target_schemaPS4.csv\"\n",
    ",r\"target_schema_metacritic.csv\"\n",
    ",r\"target_schema_Video_Games_Sales.csv\"\n",
    ",r\"wikidata_integrated_target_schema.csv\"]\n",
    "\n",
    "#path to godl folder\n",
    "pre_pro_path = r\"../data/preprocessing\".replace(\"\\\\\",\"/\")\n",
    "\n",
    "#the paths to the matches\n",
    "c_paths= [r\"A-B.csv\",\n",
    "r\"A-D.csv\",\n",
    "r\"B-C.csv\",\n",
    "r\"C-D.csv\",\n",
    "r\"C-E.csv\"]\n",
    "\n",
    "#preprocessing the pathnames for gold_stadard\n",
    "for i in range(0,len(paths)):\n",
    "    paths[i] = path_schema_csv+\"/\"+paths[i]\n",
    "\n",
    "for i in range(0,len(c_paths)):\n",
    "    c_paths[i] = gold_path+\"/\"+c_paths[i]\n",
    "\n",
    "#Naming the paths\n",
    "names = [\"B\",\"D\",\"A\",\"C\",\"E\"]\n",
    "paths = dict(zip(names,paths))\n",
    "compare = [[\"A\",\"B\"],[\"A\",\"D\"],[\"B\",\"C\"],[\"C\",\"D\"],[\"C\",\"E\"]]\n",
    "\n",
    "#function that creates .csv files from .xml files\n",
    "\n",
    "def xml_to_csv(path_source_folder, path_target_folder = \"\", attributes = ['id','name','platform','publishers','publicationDate',\n",
    "      'globallySoldUnits','genres','criticScore','userScore',\n",
    "      'developers','summary','rating','series'], list_att= [\"publishers\",\"genres\",\"developers\"] ):\n",
    "        '''\n",
    "        path_source_folder : is the path to the folder with the .xml files\n",
    "        attributes : list of names of the attributes in the .xml files\n",
    "        list_att : list of names of the attributes in attributes which are list attributes \n",
    "        '''\n",
    "        path_source_folder = path_source_folder.replace(\"\\\\\",\"/\")\n",
    "        names = [name for name in os.listdir(path_source_folder) if \".xml\" in name]\n",
    "        paths = [path_source_folder+\"/\"+name for name in names]\n",
    "\n",
    "        if (\"\"==path_target_folder):\n",
    "            path_target_folder = path_source_folder\n",
    "\n",
    "        for i,f in enumerate(paths):\n",
    "            # PARSE XML\n",
    "            #print(f)\n",
    "            xml = ElementTree.parse(f)\n",
    "\n",
    "            # CREATE CSV FILE\n",
    "            with open(path_target_folder + \"/\" + names[i].replace(\".xml\",\".csv\"), 'w', newline='',encoding=\"utf-8\") as outfile:\n",
    "                writer = csv.writer(outfile)\n",
    "                #csvfile = open(f.replace(\".xml\",\".csv\"),'w',encoding='utf-8')\n",
    "                #csvfile_writer = csv.writer(csvfile)\n",
    "\n",
    "                # ADD THE HEADER TO CSV FILE\n",
    "                #csvfile_writer.writerow([\"id\",\"name\"])\n",
    "\n",
    "                writer.writerow(attributes)\n",
    "\n",
    "                # FOR EACH EMPLOYEE\n",
    "                for i, videogame in enumerate(xml.findall(\"videogame\")):\n",
    "                    if(videogame):\n",
    "                        csv_line = list()\n",
    "                        for a in attributes:\n",
    "                                locals()[a] = videogame.find(a)\n",
    "                                #print(\"bool\",a,\":\", None != locals()[a])\n",
    "                                if (None != locals()[a]):\n",
    "                                    if (a in list_att):\n",
    "                                        sub_list = list()\n",
    "                                        for c1 in locals()[a].findall(\"*\"):\n",
    "                                            for c2 in c1.findall(\"*\"):\n",
    "                                                sub_list.append(c2.text)\n",
    "                                        csv_line.append(\",\".join(sub_list))\n",
    "                                    else: \n",
    "                                        csv_line.append(locals()[a].text)\n",
    "                                else:\n",
    "                                    csv_line.append(\"\")\n",
    "                                    #print(\"line after append:\",csv_line)\n",
    "                        # ADD A NEW ROW TO CSV FILE\n",
    "                        #csvfile_writer.writerow(csv_line)\n",
    "                        writer.writerow(csv_line)\n",
    "                        #print(\"final line:\",csv_line)\n",
    "                #csvfile.close()\n",
    "                \n",
    "#create .csv Files from the .xml files using the function\n",
    "xml_to_csv(path_source_folder=path_xml, path_target_folder=path_schema_csv)\n",
    "#save integrated scemas also as Dataset_X.csv (make shure the subfolder \"integratet_target_schema_csv\" exists)\n",
    "for n in names:\n",
    "    csv = pd.read_csv(paths[n])\n",
    "    csv.to_csv(path_schema_csv+\"/Dataset_\"+n+\".csv\", sep=\";\", index=False)\n",
    "    \n",
    "\n",
    "#find all platform names in dataset\n",
    "\n",
    "#all_platforms = [set(pd.read_csv(p)[\"platform\"]) for p in paths.values()]\n",
    "#export them\n",
    "#pd.DataFrame(all_platforms).to_csv(path_to_folder+ \"/\" + \"platforms\"+\".csv\",index=False, sep=\";\")  \n",
    "#reimport them (after they were edited and matched by hand)\n",
    "platforms = pd.read_csv(pre_pro_path+\"/platforms.csv\", sep =\";\", header=None)\n",
    "\n",
    "#transpose\n",
    "platforms = platforms.transpose()\n",
    "#select ony the ones in c (imputed for the ones in a)\n",
    "platforms = platforms.loc[~platforms.isnull()[3],range(0,5)]\n",
    "#drop the first row\n",
    "platforms = platforms.drop(0)\n",
    "#replace strings in with string lists\n",
    "for j in range (0,len(platforms.columns)):\n",
    "    for i in range(0,len(platforms)):\n",
    "        try:\n",
    "            platforms.iloc[i,j] = platforms.iloc[i,j].split(\",\")\n",
    "        except AttributeError:\n",
    "            if not math.isnan(platforms.iloc[i,j]):\n",
    "                print(\"ERROR was not detected as string, col:\",j,\"row:\",i, platforms.iloc[i,j])    \n",
    "    \n",
    "\n",
    "#change platform names, only keep the ones present in A and C (platforms[3])\n",
    "#for all datesets\n",
    "for i in range(len(names)):\n",
    "    data_csv = pd.read_csv(paths[names[i]])\n",
    "    #for all platform names\n",
    "    for j in range(0,len(platforms)):\n",
    "    #if the platform is in this dataframe\n",
    "        if type(platforms.iloc[j,i]) == list:\n",
    "            #find matching platform name and replace with entry from dataset C\n",
    "            data_csv[\"platform\"] = np.where(data_csv[\"platform\"].isin(platforms.iloc[j,i]),platforms.iloc[j,3],data_csv[\"platform\"])\n",
    "    #delete from dataset5\n",
    "    if(\"E\"== names[i]):\n",
    "        p = [item for sublist in platforms[3] for item in sublist]\n",
    "        cdata_csvsv = data_csv[data_csv[\"platform\"].isin(p)]\n",
    "    #############################################################\n",
    "    # TODO insert other preprocessing here \n",
    "    \n",
    "    \n",
    "    ##############################################################\n",
    "    #TODO save in folder preprocessed_csv\n",
    "    data_csv.to_csv(pre_pro_path+\"/uniform_platform_names/Dataset_\"+names[i]+\".csv\", index=False, sep=\";\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
